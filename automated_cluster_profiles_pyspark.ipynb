{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTEBOOK PURPOSE AND INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a dataframe derived from a clustering method (e.g. Kmeans) the below script will profile the cluster groups according to the attached features.\n",
    "\n",
    "The script will automatically determine which features are categorical, one-hot encode them and then calculate the % of the cluster that are within each category.\n",
    "\n",
    "It will also determine which features are continuous and calculate averages across the cluster group.\n",
    "\n",
    "The input file requires each record to be a unique ID, with a single column designating its cluster group. As many additional features / columns as desired can be added.\n",
    "\n",
    "While not part of the core purpsoe of the scipt there are brief examples of other useful functionality such as automatic categorical column cleaning, and discretising output features into codes.\n",
    "\n",
    "<b>This script is in pyspark (useful for large datasets that would take too long to process with python). Note there is a separate example notebook for python with a similar method).<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataiku libraries\n",
    "import dataiku\n",
    "from dataiku import spark as dkuspark\n",
    "\n",
    "client = dataiku.api_client()\n",
    "user = client.get_auth_info().get('authIdentifier', 'unknown')\n",
    "\n",
    "# Change this to your recipe name (short)\n",
    "appName = \"cluster_profiles\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Spark imports (std lib)\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext, functions as f, Window\n",
    "from pyspark.sql.functions import udf, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import QuantileDiscretizer, Bucketizer\n",
    "\n",
    "# Import custom libraries here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore::DeprecationWarning\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"\n",
    "if (sys.version_info.major, sys.version_info.minor) != (3, 6):\n",
    "    raise Exception(\"This code must be using the Python36 Spark environment.\")\n",
    "if appName == \"\":\n",
    "    raise Exception(\"Please enter a (short) recipe name for Spark\")\n",
    "\n",
    "# Override any spark configurations here\n",
    "conf = SparkConf()\n",
    "\n",
    "# Initialise the Spark Context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"{}: Recipe - {} (Py)\".format(user, appName))\n",
    "         .master(\"yarn\")\n",
    "         .config(conf=conf)\n",
    "         .getOrCreate()\n",
    "         )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read recipe inputs\n",
    "\n",
    "combined_df_hdfs = dataiku.Dataset(\"clusters_unprofiled_with_features\")\n",
    "combined_df = dkuspark.get_dataframe(sqlContext, quanta_df_hdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove any columns which have nulls greater than the threshold value specified\n",
    "sample_df = combined_df.copy()\n",
    "null_threshold = 0.25\n",
    "null_number = sample_df.count() * null_threshold\n",
    "\n",
    "null_list_collected = (\n",
    "    sample_df\n",
    "    .select([f.count(f.when(f.col(c).isNull(), c)).alias(c) for c in sample_df.columns])\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "null_list = [c[0] for c in null_list_collected[0].asDict().items() if c[1] > null_number]\n",
    "\n",
    "print(\"Dropping columns: \", null_list)\n",
    "\n",
    "sample_df = sample_df.drop(*null_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of categorical features\n",
    "categorical_cols = [c.name for c in sample_df.schema if c.dataType == StringType()]\n",
    "\n",
    "# print(\"Categorical columns: \",categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all categorical columns to remove similar values\n",
    "cat_cleaned_df = sample_df\n",
    "manual_override_list = ['categories_you_do_not_want_deleted]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    values = [tuple(c)[0] for c in cat_cleaned_df.select(col).distinct().collect()]\n",
    "    values = [v for v in values if v != None]\n",
    "    values_dedupe = list(process.dedupe(values, threshold=99))\n",
    "    values_removed = [v for v in values if v not in values_dedupe]\n",
    "\n",
    "    if len(values_removed) > 0:\n",
    "        replacements_dict = {}\n",
    "        for removed_value in values_removed:\n",
    "            replacement_value = process.extract(removed_value, values_dedupe, limit=1)[0][0]\n",
    "            replacements_dict[removed_value] = replacement_value\n",
    "\n",
    "        for item in replacements_dict:\n",
    "            if item not in manual_override_list:\n",
    "                print(f\"Column: {col}, replacing: {item} with {replacements_dict[item]}\")\n",
    "                cat_cleaned_df = cat_cleaned_df.withColumn(col,\n",
    "                        f.when(f.col(col) == item, f.lit(replacements_dict[item])).otherwise(f.col(col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE HOT ENCODING OF CATEGORICAL FEATURES\n",
    "def clean_category(cat):\n",
    "    cat = cat.replace(\"/\", \"_\")\n",
    "    cat = cat.replace(\" \", \"-\")\n",
    "    return cat\n",
    "\n",
    "encoded_df = cat_cleaned_df\n",
    "\n",
    "# Loop through the columns and do a manual one-hot encoding\n",
    "for col in categorical_cols:\n",
    "    rows = cat_cleaned_df.select(col).distinct().collect()\n",
    "    for category in [tuple(c)[0] for c in rows]:\n",
    "        if category is not None:\n",
    "            # Replace certain chars\n",
    "            category_str = clean_category(category)\n",
    "            encoded_df = (\n",
    "                encoded_df\n",
    "                .withColumn(f\"{col}_{category_str}\",\n",
    "                            f.when(f.col(col) == category, f.lit(1))\n",
    "                            .otherwise(f.lit(0)))\n",
    "            )\n",
    "    # Remove the original column (no longer needed)\n",
    "    encoded_df = encoded_df.drop(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert BooleanType() columns to IntegerType() - required for aggregate function\n",
    "bool_fix_list = [c.name for c in encoded_df.schema if c.dataType == BooleanType()]\n",
    "for col in bool_fix_list:\n",
    "    encoded_df = encoded_df.withColumn(col, f.col(col).cast(IntegerType()))\n",
    "\n",
    "# Create the dictionary of columns on which we wish to aggregate\n",
    "indexfield = \"unique_identifier\"\n",
    "groupbyfield = \"cluster_group\"\n",
    "\n",
    "# Get a list of columns that are boolean in nature that should be summed in aggregate function\n",
    "sum_cols = []\n",
    "for col in encoded_df.columns:\n",
    "    values = [tuple(c)[0] for c in encoded_df.select(col).distinct().collect()]\n",
    "    if values in ([1,0], [0,1], [0], [1]):\n",
    "        sum_cols.append(col)\n",
    "\n",
    "# Get list of integer and float columns that will be averaged when aggregated\n",
    "avg_cols = [c.name for c in encoded_df.schema\n",
    "            if c.dataType in [IntegerType(), DoubleType(), LongType()] \n",
    "            and c.name not in [indexfield, groupbyfield, *sum_cols]]\n",
    "\n",
    "# Create a dictionary of aggregate levels for each column\n",
    "agg_dict = {}\n",
    "for col in avg_cols:\n",
    "    agg_dict[col] = \"avg\"\n",
    "for col in sum_cols:\n",
    "    agg_dict[col] = \"sum\"\n",
    "agg_dict[indexfield] = \"count\"\n",
    "\n",
    "print(\"Features summed: \", sum_cols, \"\\n\")\n",
    "print(\"Features averaged: \", avg_cols, \"\\n\")\n",
    "print(\"Features counted: \", indexfield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate up by cluster\n",
    "grouped_df = encoded_df.groupBy(groupbyfield).agg(agg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'avg' columns\n",
    "cols_to_rename = [c for c in grouped_df.columns if \"avg(\" in c]\n",
    "for col in cols_to_rename:\n",
    "    rename = col[4:-1]  # remove the `avg()` from the name\n",
    "    grouped_df = grouped_df.withColumnRenamed(col, f\"{rename}_avg\")\n",
    "\n",
    "# Rename the 'sum' columns\n",
    "cols_to_rename = [c for c in grouped_df.columns if \"sum(\" in c]\n",
    "for col in cols_to_rename:\n",
    "    rename = col[4:-1]  # remove the `sum()` from the name\n",
    "    grouped_df = grouped_df.withColumnRenamed(col, f\"{rename}\")\n",
    "\n",
    "# Rename the 'count' column\n",
    "cols_to_rename = [c for c in grouped_df.columns if \"count(\" in c]\n",
    "for col in cols_to_rename:\n",
    "    rename = col[6:-1]  # remove the `count()` from the name\n",
    "    grouped_df = grouped_df.withColumnRenamed(col, f\"{rename}_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn absolute values into % values by cluster\n",
    "for col in sum_cols:\n",
    "    grouped_df = (grouped_df.withColumn(col,\n",
    "                            (f.col(col) * 100 / f.col(f\"{indexfield}_num\")))\n",
    "                            .withColumnRenamed(col, f\"{col}_pct\")\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate % of total population column\n",
    "sample_size = grouped_df.agg(f.sum(f\"{indexfield}_num\")).collect()[0][0]\n",
    "\n",
    "grouped_df = grouped_df.withColumn(f\"{indexfield}_pct\",\n",
    "            (f.col(f\"{indexfield}_num\") * 100 / sample_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split continuous feature into 5 codes using quintiles, C1 is low, 5 is high\n",
    "example_discretizer = QuantileDiscretizer(numBuckets=5, inputCol=\"continuous_feature_example\", outputCol=\"feature_code\")\n",
    "\n",
    "grouped_df = example_discretizer.fit(grouped_df).setHandleInvalid(\"keep\").transform(grouped_df)\n",
    "\n",
    "# Set mapping of code bins\n",
    "t = {0.0: \"C1\",\n",
    "      1.0: \"C2\",\n",
    "      2.0: \"C3\",\n",
    "      3.0: \"C4\",\n",
    "      4.0: \"C5\"}\n",
    "\n",
    "udf_rename = udf(lambda x: t[x], StringType())\n",
    "grouped_df = grouped_df.withColumn(\"feature_code\", udf_rename(\"feature_code\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split continuous feature into 3 codes using pre defined limits, C1 is low, 3 is high\n",
    "\n",
    "grouped_df = grouped_df.withColumn(\"num_amenities_avg\", num_amenities)\n",
    "\n",
    "# Split example data into 3 groups\n",
    "splits = [0, 1200, 2600, float(\"inf\")]\n",
    "\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"num_amenities_avg\", outputCol=\"test_code\")\n",
    "\n",
    "grouped_df = bucketizer.transform(grouped_df)\n",
    "\n",
    "# Set mapping of churn bins\n",
    "t = {0.0:\"low\",\n",
    "      1.0: \"med\",\n",
    "      2.0:\"high\"}\n",
    "\n",
    "udf_rename = udf(lambda x: t[x], StringType())\n",
    "grouped_df = grouped_df.withColumn(\"test_code\", udf_rename(\"test_code\"))\n",
    "grouped_df = grouped_df.drop(\"num_amenities_avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add partition to profiles\n",
    "date_value = quanta_df.limit(1).select(\"date\").collect()[0][\"date\"]\n",
    "\n",
    "# Create date partition column based on value in input table\n",
    "grouped_df = grouped_df.withColumn(\"date\", f.lit(date_value))"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "compute_Profiles_step1",
  "creator": "thunt",
  "customFields": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
